{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9P6DwVFOZSr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxF-RAxfNC61"
      },
      "source": [
        "# Combining Multiple Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spXTOhYUNOyH"
      },
      "source": [
        "## Image to Text + Text to Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fYC_coeNdGz"
      },
      "outputs": [],
      "source": [
        "# import PIL\n",
        "import torch\n",
        "from PIL import Image, ImageOps\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from diffusers import StableDiffusionPipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsyyzO1HKXN2"
      },
      "source": [
        "models\n",
        "\n",
        "https://huggingface.co/Salesforce/blip-image-captioning-base\n",
        "\n",
        "https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EHzB6tuNfRn"
      },
      "outputs": [],
      "source": [
        "image_to_text_model = \"Salesforce/blip-image-captioning-base\"\n",
        "text_to_image_model = \"runwayml/stable-diffusion-v1-5\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wykj_2MLmZ7"
      },
      "source": [
        "- `BlipProcessor` prepares images for the model\n",
        "\n",
        "- `BlipForConditionalGeneration` generates text descriptions for images\n",
        "\n",
        "- `.to(\"mps\")` moves the model to the Apple Metal Performance Shaders (MPS) device for GPU acceleration on macOS\n",
        "- can use \"cuda\" alternatively on PC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4_f-Xf8NhuK"
      },
      "outputs": [],
      "source": [
        "processor = BlipProcessor.from_pretrained(image_to_text_model)\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(image_to_text_model)\n",
        "blip_model.to(\"mps\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1spSKQqpNlCp"
      },
      "outputs": [],
      "source": [
        "# load text to image model\n",
        "pipe = StableDiffusionPipeline.from_pretrained(text_to_image_model, torch_dtype=torch.float16, safety_checker=None)\n",
        "pipe.to(\"mps\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eslr7R5HTmc5"
      },
      "source": [
        "load image (right side up and at most of size 512x512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuilHtVNNkp6"
      },
      "outputs": [],
      "source": [
        "file_path = \"./photo.jpg\"\n",
        "\n",
        "def load_image(file_path_, max_size=(512, 512)):\n",
        "    img = Image.open(file_path_)\n",
        "    img = ImageOps.exif_transpose(img)\n",
        "    img = img.convert(\"RGB\")\n",
        "    img.thumbnail(max_size)\n",
        "    return img\n",
        "\n",
        "image = load_image(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4Mf5IacTlz6"
      },
      "source": [
        "- prepare image for blip model by converting it into a pytorch tensor format\n",
        "\n",
        "- `blip_model.generate` generates image caption\n",
        "\n",
        "  - `num_beams` controls the number of beams used for beam search, a decoding strategy to improve output quality\n",
        "\n",
        "- `processor.decode` decodes the generated output to a human-readable string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kF0SS4PcNtC4"
      },
      "outputs": [],
      "source": [
        "# getting description from img\n",
        "inputs = processor(image, return_tensors=\"pt\").to(\"mps\")\n",
        "outputs = blip_model.generate(**inputs,num_beams=20)\n",
        "description = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"Description: {description}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnUJxQlGUBZ1"
      },
      "source": [
        "- `num_inference_steps=25` controls number of steps in the diffusion process (higher values may improve quality but increase processing time)\n",
        "\n",
        "- `image_guidance_scale=1` determines how much the description should influence the final output (higher values make the output more closely match the description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sF4Zy0ySM6M9"
      },
      "outputs": [],
      "source": [
        "modified_description = f\"A Disney-style portrait of {description} with large, expressive eyes and a whimsical smile.\"\n",
        "\n",
        "# new image from modified description\n",
        "generated_image = pipe(modified_description, num_inference_steps=25, guidance_scale=20).images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KYW_A3uNxei"
      },
      "outputs": [],
      "source": [
        "output_path = \"./output_imagev2.jpg\"\n",
        "generated_image.save(output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni9EPQ9vNXTs"
      },
      "source": [
        "## Image to Text + Image to Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gq3pspGYN1I3"
      },
      "outputs": [],
      "source": [
        "# import PIL\n",
        "import torch\n",
        "from PIL import Image, ImageOps\n",
        "from diffusers import StableDiffusionInstructPix2PixPipeline, EulerAncestralDiscreteScheduler\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnL2qe3aN_BE"
      },
      "source": [
        "models\n",
        "\n",
        "https://huggingface.co/Salesforce/blip-image-captioning-base\n",
        "\n",
        "https://huggingface.co/timbrooks/instruct-pix2pix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7X7lk-2RN2VJ"
      },
      "outputs": [],
      "source": [
        "image_to_text_model = \"Salesforce/blip-image-captioning-base\"\n",
        "image_to_image_model = \"timbrooks/instruct-pix2pix\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haACNjd5OP37"
      },
      "source": [
        "- `BlipProcessor` prepares images for the model\n",
        "\n",
        "- `BlipForConditionalGeneration` generates text descriptions for images\n",
        "\n",
        "- `.to(\"cpu\")` moves the model to cpu\n",
        "\n",
        "- can use `\"mps\"` for GPU acceleration on macOS\n",
        "- can use `\"cuda\"` alternatively on PC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEo4aVTnN4WF"
      },
      "outputs": [],
      "source": [
        "# load image to text model\n",
        "processor = BlipProcessor.from_pretrained(image_to_text_model)\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(image_to_text_model)\n",
        "blip_model.to(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpm8J9uSOW7O"
      },
      "source": [
        "`EulerAncestralDiscreteScheduler`: A scheduler is used to guide the image generation process by determining how noise is added and removed during the diffusion steps\n",
        "\n",
        "Starts with a noisy, random image and gradually reduces the noise step by step\n",
        "\n",
        "As the noise is removed, model refines image based on prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rZyjQsdN5uW"
      },
      "outputs": [],
      "source": [
        "# load text to image model\n",
        "pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(image_to_image_model, torch_dtype=torch.float16, safety_checker=None)\n",
        "pipe.to(\"cpu\")\n",
        "pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajK75JTRRU_b"
      },
      "source": [
        "load image (right side up and at most of size 512x512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drmCB7bYN8mA"
      },
      "outputs": [],
      "source": [
        "file_path = \"photo.jpg\"\n",
        "\n",
        "def load_image(file_path_, max_size=(512, 512)):\n",
        "    img = Image.open(file_path_)\n",
        "    img = ImageOps.exif_transpose(img)\n",
        "    img = img.convert(\"RGB\")\n",
        "    img.thumbnail(max_size)\n",
        "    return img\n",
        "\n",
        "image = load_image(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eydR3_MMRu2x"
      },
      "source": [
        "- prepare image for blip model by converting it into a pytorch tensor format\n",
        "\n",
        "- `blip_model.generate` generates image caption\n",
        "\n",
        "  - `num_beams` controls the number of beams used for beam search, a decoding strategy to improve output quality\n",
        "\n",
        "- `processor.decode` decodes the generated output to a human-readable string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KalBc75vN-PJ"
      },
      "outputs": [],
      "source": [
        " # getting description from image\n",
        "inputs = processor(image, return_tensors=\"pt\").to(\"cpu\")\n",
        "outputs = blip_model.generate(**inputs, num_beams=20)\n",
        "description = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"Description: {description}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "remzVoxITCY_"
      },
      "source": [
        "- `num_inference_steps=25` controls number of steps in the diffusion process (higher values may improve quality but increase processing time)\n",
        "\n",
        "- `image_guidance_scale=1` determines how much the original image should influence the final output (lower values rely more on the original)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqkLbsD0M-tg"
      },
      "outputs": [],
      "source": [
        "modified_description = f\"Create a Disney-style portrait of {description} with large, expressive eyes and a whimsical smile.\"\n",
        "\n",
        "# new image from modified description\n",
        "images = pipe(modified_description, image=image, num_inference_steps=25, image_guidance_scale=1).images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JvhY1YLN__K"
      },
      "outputs": [],
      "source": [
        "output_path = \"./output_image.jpg\"\n",
        "images[0].save(output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cER5wa1OOb90"
      },
      "source": [
        "# Inpainting"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load the image\n",
        "image = cv2.imread('inpaint.png')\n",
        "original_image = image.copy()\n",
        "mask = np.zeros(image.shape[:2], dtype=np.uint8)  # Mask initialized as black\n",
        "drawing = False  # True if the user is drawing\n",
        "points = []  # Store points to create the polygon\n",
        "\n",
        "# Mouse callback function\n",
        "def draw_polygon(event, x, y, flags, param):\n",
        "    global drawing, points, mask\n",
        "\n",
        "    if event == cv2.EVENT_LBUTTONDOWN:  # Start drawing on left mouse button down\n",
        "        drawing = True\n",
        "        points = [(x, y)]  # Initialize points with the starting point\n",
        "\n",
        "    elif event == cv2.EVENT_MOUSEMOVE:\n",
        "        if drawing:\n",
        "            points.append((x, y))  # Add points as the mouse moves\n",
        "            cv2.line(image, points[-2], points[-1], (0, 255, 0), 2)  # Draw green lines\n",
        "\n",
        "    elif event == cv2.EVENT_LBUTTONUP:  # Stop drawing on left mouse button up\n",
        "        drawing = False\n",
        "        points.append((x, y))\n",
        "\n",
        "        # Draw final line to close the shape\n",
        "        cv2.line(image, points[-1], points[0], (0, 255, 0), 2)\n",
        "\n",
        "        # Fill the polygon on the mask\n",
        "        points_array = np.array(points, dtype=np.int32)\n",
        "        cv2.fillPoly(mask, [points_array], 255)  # Fill inside of the drawn shape with white on the mask\n",
        "\n",
        "# Set up window and bind mouse callback\n",
        "cv2.namedWindow('Draw Mask')\n",
        "cv2.setMouseCallback('Draw Mask', draw_polygon)\n",
        "\n",
        "while True:\n",
        "    cv2.imshow('Draw Mask', image)\n",
        "\n",
        "    # Display the masked result in real-time\n",
        "    masked_result = np.where(mask[:, :, None] == 255, 255, 0).astype(np.uint8)\n",
        "    cv2.imshow('Masked Result', masked_result)\n",
        "\n",
        "    key = cv2.waitKey(1)\n",
        "\n",
        "    if key == ord('r'):  # Press 'r' to reset the drawing\n",
        "        image = original_image.copy()\n",
        "        mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
        "\n",
        "    elif key == ord('q'):  # Press 'q' to quit and save\n",
        "        break\n",
        "\n",
        "# Save the final mask\n",
        "cv2.imwrite('inpaint_mask.png', mask)\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "FFn7Cxt6awP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import AutoPipelineForInpainting\n",
        "from diffusers.utils import load_image, make_image_grid\n",
        "\n",
        "# Load the inpainting model with half-precision for efficiency\n",
        "pipeline = AutoPipelineForInpainting.from_pretrained(\n",
        "    \"kandinsky-community/kandinsky-2-2-decoder-inpaint\", torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Load the base image and mask (white pixels define areas to modify)\n",
        "init_image = load_image(\"inpaint.png\")\n",
        "mask_image = load_image(\"inpaint_mask.png\")\n",
        "\n",
        "# Offload model layers to CPU when not in use to save memory\n",
        "pipeline.enable_model_cpu_offload()\n",
        "\n",
        "# Set up random generator for reproducible results\n",
        "generator = torch.Generator(\"cuda\").manual_seed(92)\n",
        "\n",
        "# Run the inpainting model with the given prompt\n",
        "prompt = \"concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k\"\n",
        "image = pipeline(prompt=prompt, image=init_image, mask_image=mask_image).images[0]\n",
        "\n",
        "# Display the resulting image in a grid\n",
        "make_image_grid([image], rows=1, cols=1)\n"
      ],
      "metadata": {
        "id": "Xs0K_LI5POJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Model Selection and Loading:**\n",
        "`AutoPipelineForInpainting` loads a pre-trained inpainting model. In this case, it’s the `Kandinsky-2-2 Decoder` model, suitable for creating detailed, AI-assisted modifications to images.\n",
        "Setting `torch_dtype` to `torch.float16` helps optimize memory usage by loading the model in half-precision.\n",
        "2. **Image and Mask Preparation:**\n",
        "`init_image` is the original image we want to inpaint or modify, and `mask_image` is the guide mask image. Areas marked in white in the mask image are eligible for modification based on the prompt.\n",
        "\n",
        "3. **Generator for Reproducibility:**\n",
        "By using `torch.Generator` with a fixed seed, we ensure that every run with the same seed produces identical results\n",
        "\n",
        "4. **Running the Inpainting Process:**\n",
        "The pipeline generates an output image based on the input prompt and the provided mask, blending the prompt’s style into the inpainted areas of init_image.\n",
        "\n",
        "5. **Image Display:**\n",
        "make_image_grid creates a grid layout to visualize the output image(s). Here it’s a 1x1 grid, displaying only the generated result."
      ],
      "metadata": {
        "id": "wo0csrvgRkwA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image-to-image"
      ],
      "metadata": {
        "id": "QrKYJyzR1un6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image, ImageOps\n",
        "from diffusers import StableDiffusionInstructPix2PixPipeline, EulerAncestralDiscreteScheduler\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration"
      ],
      "metadata": {
        "id": "R_d8BXNN2FcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```StableDiffusionInstructPix2PixPipeline``` -> This is a pipeline for the model ***InstructPix2Pix*** which takes an image and modifies it based on a specified prompt"
      ],
      "metadata": {
        "id": "Iil3uRZeBbCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "model_id = \"timbrooks/instruct-pix2pix\"\n",
        "pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16, safety_checker=None)\n",
        "pipe.to(\"cpu\")\n",
        "pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)"
      ],
      "metadata": {
        "id": "JZxuogwm2RQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "loads and edits the image as needed"
      ],
      "metadata": {
        "id": "jpOltbKrBO0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"photo.jpg\"\n",
        "\n",
        "def load_image(file_path_, max_size=(512, 512)):\n",
        "    img = Image.open(file_path_)\n",
        "    img = ImageOps.exif_transpose(img)\n",
        "    img = img.convert(\"RGB\")\n",
        "    img.thumbnail(max_size)\n",
        "    return img\n",
        "\n",
        "image = load_image(file_path)"
      ],
      "metadata": {
        "id": "UIYBnVmu2jx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Promps for human / or clearly identifiable subjects"
      ],
      "metadata": {
        "id": "Vxsolm5I3QZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#prompt = \"Modify subject into a western style character\"\n",
        "#prompt = \"Modify the subject to resemble a gigachad, strong jawline, defined cheekbones, ensure he remains recognizable. Keep the background unchanged and the overall expression natural.\"\n",
        "#prompt = \"Modify subject as a disney movie character style, large expressive eyes, whimsical smile, preserve the original face shape, preserve clothes\"\n",
        "#prompt = \"Modify the subject to resemble a comic book character, preserve face shape, preserve background\"\n",
        "#prompt = \"as a comic\"\n",
        "#prompt = \"Modify the subject to resemble an anime style character, preserve face shape\"\n",
        "#prompt = \"as an anime\"\n",
        "#prompt = \"as an astronaut\""
      ],
      "metadata": {
        "id": "vVVQ1yQl2pn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompts for general picture style / Non human photos"
      ],
      "metadata": {
        "id": "VVbvKIrS3CJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#prompt = \"make the subject a cartoon like character\"\n",
        "#prompt = \"add red flowers in the background\"\n",
        "#prompt = \"make the subject a dark brown, keep mouth and toungue unchanged\"\n",
        "#prompt = \"make the picture a van gogh style painting\"\n",
        "#prompt = \"as a basquiat style painting\"\n",
        "#prompt = \"as an anime\""
      ],
      "metadata": {
        "id": "EK-Ec49B2sYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "negative prompt"
      ],
      "metadata": {
        "id": "iqSJ9WYO3U72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neg = \"distortions, unrecognizable features unnatural characteristics, bad anatomy, subject not recognizable\""
      ],
      "metadata": {
        "id": "C1Bn0KQG2vdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = pipe(prompt, negative_prompt=neg, image=image, num_inference_steps=25, guidance_scale=5.5, image_guidance_scale=1.5).images\n",
        "output_path = \"./output_image2.jpg\"\n",
        "images[0].save(output_path)"
      ],
      "metadata": {
        "id": "HH_Sth2E2z44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```prompt``` -> the prompt that gives instructions to the model\n",
        "\n",
        "```negative_prompt = neg``` -> negative prompt specified with undesired characteristics in the resulting image\n",
        "\n",
        "```image = image``` -> image we want to modify\n",
        "\n",
        "```num_inference_steps = 25``` -> number of **denoising steps** (changes the quality of the output image - the more steps lead to higher quality)\n",
        "\n",
        "```guidance_scale = 5.5``` -> how closely followed the prompt we specified will be (higher value = prompt will be followed more closely, lower value = model has more \"freedom\")\n",
        "\n",
        "```image_guidance_scale= 1.5``` -> How similar should de output image be to teh original image (higher values lead to more similar outputs)\n"
      ],
      "metadata": {
        "id": "OiaUuq3Brl0y"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}